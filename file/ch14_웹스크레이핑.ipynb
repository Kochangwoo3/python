{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442fe6bb-46fd-41a4-a38a-f38d064370d6",
   "metadata": {},
   "source": [
    "# 웹 스크레이핑\n",
    "- 컴퓨터 소프트웨어 기술을 활용해 웹 사이트 내에 있는 정보를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa78178-c6b6-4fdd-9b63-7769ed5ed904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "url = 'www.naver.com'\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c050d5b3-c7cb-4832-b6e1-f54a8d2072a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_serarch_url = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=\"\n",
    "search_word = '파이썬'\n",
    "url = naver_serarch_url + search_word\n",
    "\n",
    "webbrowser.open_new(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28b94754-0dde-48f6-be8f-8ab5e287d1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_url = \"https://www.google.com/search?q=\"\n",
    "search_word = '환율'\n",
    "url = google_url + search_word\n",
    "webbrowser.open_new(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "102c6c42-a5a5-445a-b7c8-4458149973d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 개의 웹 사이트에 접속하기\n",
    "urls = ['www.naver.com', 'www.daum.net', 'www.google.com']\n",
    "for url in urls:\n",
    "    webbrowser.open_new(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859deda-0f4b-4a50-9977-e8410a856822",
   "metadata": {},
   "source": [
    "# 웹 스크레이피을 위한 기본 지식\n",
    "- HTTP: 인터넷상에서 html문서의 정보를 주고 받을 수 있도록 만든 프로토콜(전송 규약) 입니다.\n",
    "- HTML: 웹페이지 의 구조적 구성을 위한 언어\n",
    "- 웹페이지: 웹 상에 있는 HTML로 구성된 개별문서, 보통 하나의 웹사이트는 여러 개의 웹페이지로 구성합니다.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38706b7d-def6-4a31-8f0d-97deb343e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing D:/dev/myPyCode/HTML_example.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile D:/dev/myPyCode/HTML_example.html\n",
    "<!doctype html>\n",
    "<html>\n",
    " <head>\n",
    "  <meta charset=\"utf-8\">\n",
    "  <title>이것은 HTML 예제</title>\n",
    " </head>\n",
    " <body>\n",
    "  <h1>출간된 책 정보</h1>\n",
    "  <p id=\"book_title\">이해가 쏙쏙 되는 파이썬</p>\n",
    "  <p id=\"author\">홍길동</p>\n",
    "  <p id=\"publisher\">위키북스 출판사</p>\n",
    "  <p id=\"year\">2018</p>\n",
    " </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2070aa-f571-4c09-8db0-b100c73cb7ab",
   "metadata": {},
   "source": [
    "## 웹페이지의 HTML 소스 갖고 오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b531a73e-d33f-447a-aeed-639d1b802d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get(\"https://www.google.co.kr\")\n",
    "# r 응답 객체\n",
    "print(r.text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f4b91e0-b314-4c03-86d7-47a66f8aa0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <span>\n",
      "    <a href=\"http://www.naver.com\">\n",
      "     naver\n",
      "    </a>\n",
      "    <a href=\"https://www.google.com\">\n",
      "     google\n",
      "    </a>\n",
      "    <a href=\"http://www.daum.net/\">\n",
      "     daum\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# 추출\n",
    "# 테스트용 html 코드\n",
    "html = \"\"\"<html><body><div><span>\\\n",
    "        <a href=http://www.naver.com>naver</a>\\\n",
    "        <a href=https://www.google.com>google</a>\\\n",
    "        <a href=http://www.daum.net/>daum</a>\\\n",
    "        </span></div></body></html>\"\"\"  \n",
    "# BeautifulSoup 를 이용해 html 소스를 파싱\n",
    "soup = BeautifulSoup(html, 'lxml')  # lxml : html소스를 처리하기 위한 파서\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a9eb17f-02f0-4c64-8eca-20ae37e56e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://www.naver.com\">naver</a>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbbebbfd-9c2d-442d-8c89-e55cc60bcc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naver'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f66020ea-4021-4799-b363-5156c3a006ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.naver.com\">naver</a>,\n",
       " <a href=\"https://www.google.com\">google</a>,\n",
       " <a href=\"http://www.daum.net/\">daum</a>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae94c7c1-e967-435a-9671-dbf8e3a3f6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver\n",
      "google\n",
      "daum\n"
     ]
    }
   ],
   "source": [
    "site_names = soup.find_all('a')\n",
    "for site_name in site_names:\n",
    "    print(site_name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f733bf61-e6b7-4fe0-b433-1976a7cef20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 테스트용 HTML 코드\n",
    "html2 = \"\"\"\n",
    "<html>\n",
    " <head>\n",
    "  <title>작품과 작가 모음</title>\n",
    " </head>\n",
    " <body>\n",
    "  <h1>책 정보</h1>\n",
    "  <p id=\"book_title\">토지</p>\n",
    "  <p id=\"author\">박경리</p>\n",
    "  \n",
    "  <p id=\"book_title\">태백산맥</p>\n",
    "  <p id=\"author\">조정래</p>\n",
    "\n",
    "  <p id=\"book_title\">감옥으로부터의 사색</p>\n",
    "  <p id=\"author\">신영복</p>\n",
    " </body>\n",
    "</html>\n",
    "\"\"\" \n",
    "\n",
    "soup2 = BeautifulSoup(html2, 'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b96ff85-8084-4df3-b319-fee734491b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>작품과 작가 모음</title>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dadec479-f1f3-496d-88b6-da50a678c81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<h1>책 정보</h1>\n",
       "<p id=\"book_title\">토지</p>\n",
       "<p id=\"author\">박경리</p>\n",
       "<p id=\"book_title\">태백산맥</p>\n",
       "<p id=\"author\">조정래</p>\n",
       "<p id=\"book_title\">감옥으로부터의 사색</p>\n",
       "<p id=\"author\">신영복</p>\n",
       "</body>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9eb9742-4eb2-4018-8a5c-15ee5373f032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>책 정보</h1>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad75514b-b79b-44b9-9838-f243a319b585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2317164-aa3c-4c76-a3a9-6951ba2087dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"book_title\">토지</p>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find('p', {\"id\":\"book_title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef9aa3dc-0476-4ffc-ad24-3f2db44461cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토지 / 박경리\n",
      "태백산맥 / 조정래\n",
      "감옥으로부터의 사색 / 신영복\n"
     ]
    }
   ],
   "source": [
    "# 책제목  / 저자\n",
    "from bs4 import BeautifulSoup\n",
    "soup2 = BeautifulSoup(html2, \"lxml\")\n",
    "\n",
    "book_titles = soup2.find_all('p', {'id':'book_title'})\n",
    "authors = soup2.find_all('p', {'id':'author'})\n",
    "\n",
    "for book_title, author in zip(book_titles, authors):\n",
    "    print(book_title.get_text() + ' / ' + author.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8212214-bea9-47e7-ad74-f17f7ed7ff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing D:/dev/myPyCode/HTML_example_my_site.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile D:/dev/myPyCode/HTML_example_my_site.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>사이트 모음</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p id=\"title\"><b>자주 가는 사이트 모음</b></p>\n",
    "    <p id=\"contents\">이곳은 자주 가는 사이트를 모아둔 곳입니다.</p>\n",
    "    <a href=\"http://www.naver.com\" class=\"portal\" id=\"naver\">네이버</a> <br>\n",
    "    <a href=\"https://www.google.com\" class=\"search\" id=\"google\">구글</a> <br>\n",
    "    <a href=\"http://www.daum.net\" class=\"portal\" id=\"daum\">다음</a> <br>\n",
    "    <a href=\"http://www.nl.go.kr\" class=\"government\" id=\"nl\">국립중앙도서관</a>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29035d01-ba7b-46b5-8525-81f3d180341d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"search\" href=\"https://www.google.com\" id=\"google\">구글</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>,\n",
       " <a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"D:/dev/myPyCode/HTML_example_my_site.html\", encoding='utf-8')\n",
    "html3 = f.read()\n",
    "f.close()\n",
    "\n",
    "soup3 = BeautifulSoup(html3, 'lxml')\n",
    "soup3.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0799e2d9-740d-4110-802c-8ccb6134d4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"daum\">다음</a>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a.portal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0000945e-6526-4e2d-8cf3-2f6d466f9018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a#naver')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9de57-88d6-4d92-8af1-0f5ea48c8691",
   "metadata": {},
   "source": [
    "## 줄 바꿈으로 가독성 높이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c80f5c-97a2-4fb4-8437-dda2cf1a7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing c:/dev/myPyCode/br_example_constitution.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile c:/dev/myPyCode/br_example_constitution.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>줄 바꿈 테스트 예제</title>\n",
    "  </head>\n",
    "  <body>\n",
    "  <p id=\"title\"><b>대한민국헌법</b></p>\n",
    "  <p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
    "  <p id=\"content\">제2조 <br/>①대한민국의 국민이 되는 요건은 법률로 정한다.<br/>②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.</p>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d43b16-8930-48ee-b65e-aab6edf3cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "제1조 ①대한민국은 민주공화국이다.②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "제2조 ①대한민국의 국민이 되는 요건은 법률로 정한다.②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "f = open('c:/dev/myPyCode/br_example_constitution.html', encoding='utf-8')\n",
    "html_source = f.read()\n",
    "f.close()\n",
    "\n",
    "soup = BeautifulSoup(html_source, 'lxml')\n",
    "\n",
    "title = soup.find('p', {\"id\":\"title\"})\n",
    "contents = soup.find_all('p', {\"id\":\"content\"})\n",
    "\n",
    "print(title.get_text())\n",
    "for content in contents:\n",
    "    print(content.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb04d436-b980-4600-a701-2834a395408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> 태그 p로 찾은 요소\n",
      "<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
      "===> 결과에서 태그 br로 찾은 요소:  <br/>\n",
      "===> 태그 br을 개행문자로 바꾼 결과\n",
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "html1 = '<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>'\n",
    "\n",
    "soup1 = BeautifulSoup(html1, \"lxml\")\n",
    "print(\"===> 태그 p로 찾은 요소\")\n",
    "content1 = soup1.find('p', {\"id\":\"content\"})\n",
    "print(content1)\n",
    "br_content = content1.find(\"br\")\n",
    "print(\"===> 결과에서 태그 br로 찾은 요소: \", br_content)\n",
    "\n",
    "br_content.replace_with(\"\\n\")\n",
    "print(\"===> 태그 br을 개행문자로 바꾼 결과\")\n",
    "print(content1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e03ed9ee-f96a-405b-bac4-c7a7281d7a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(html1, \"lxml\")\n",
    "content2 = soup2.find('p', {\"id\":\"content\"})\n",
    "\n",
    "br_contents = content2.find_all(\"br\")\n",
    "# print(br_contents)\n",
    "for br_content in br_contents:\n",
    "    br_content.replace_with(\"\\n\")\n",
    "print(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b065ebb0-62ac-4f87-829b-ce1ba17d4676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n"
     ]
    }
   ],
   "source": [
    "# 기능을 함수로\n",
    "def replace_newline(soup_html):\n",
    "    br_to_newlines = soup_html.find_all(\"br\")\n",
    "    for br_to_newline in br_to_newlines:\n",
    "        br_to_newline.replace_with(\"\\n\")\n",
    "    return soup_html\n",
    "\n",
    "# 함수를 이용한 결과에서 요소의 내용만  추출 get_text()\n",
    "soup2 = BeautifulSoup(html1, \"lxml\")\n",
    "content2 = soup2.find('p', {\"id\":\"content\"})\n",
    "content3 = replace_newline(content2)\n",
    "print(content3.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8878b5f0-ee1b-472b-a9b2-be2b273ef1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법 \n",
      "\n",
      "제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다. \n",
      "\n",
      "제2조 \n",
      "①대한민국의 국민이 되는 요건은 법률로 정한다.\n",
      "②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_source, \"lxml\")\n",
    "\n",
    "title = soup.find('p', {\"id\":\"title\"})\n",
    "contents = soup.find_all('p', {\"id\":\"content\"})\n",
    "\n",
    "print(title.get_text(), '\\n')\n",
    "\n",
    "for content in contents:\n",
    "    content1 = replace_newline(content)\n",
    "    print(content1.get_text(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42282a-b7fd-4e0b-928a-0ff2a291bbb5",
   "metadata": {},
   "source": [
    "# 14.3 웹 사이트에서 데이터 가져오기\r\n",
    "- 주의사항: 너무 빈번한 접근은 해당 웹사이트 서버 부담 -\r\n",
    "- 웹사이트 주소 변경되기도 해서 안될수도 있다\r\n",
    "- 인터넷 공개된 데이타 저작권이 있는 경우 있어 활용하기 전에 저작권 침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89fdd55c-3d2a-4880-8e67-9f5b3c1dd9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024년 9월 대한민국에서 상위 웹사이트]\n",
      "1위: youtube.com\n",
      "2위: google.com\n",
      "3위: naver.com\n",
      "4위: yako.red\n",
      "5위: tistory.com\n",
      "6위: x.com\n",
      "7위: inven.co.kr\n",
      "8위: instagram.com\n",
      "9위: newtoki346.com\n",
      "10위: newtoki345.com\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "kind = \"kr\"\n",
    "nation = {\"kr\":\"대한민국\", \"us\":\"미국\", \"jp\":\"일본\"}\n",
    "\n",
    "url = \"https://ko.semrush.com/trending-websites/{0}/all\".format(kind)\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "# tbody 태그요소 하위에 a태그 요소\n",
    "website_ranking = soup_website_ranking.select('tbody a')\n",
    "#website_ranking[0:5]\n",
    "#website_ranking[0].get_text()\n",
    "#for문을 적용한 리스트 컴프리헨션 이용\n",
    "website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking]\n",
    "\n",
    "print('[2024년 9월 {0}에서 상위 웹사이트]'.format(nation[kind]))\n",
    "for k in range(10):  # [0,1,2,3,4,5,6,7,8,9]\n",
    "    print(\"{0}위: {1}\".format(k+1, website_ranking_address[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24df636-efef-4aca-abe2-4e631c7900fa",
   "metadata": {},
   "source": [
    "# 벅스 주간 음악순위 웹 스크레이핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b93dc6e-e438-4692-8250-68c82da58cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Supernatural', 'NewJeans']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 벅스 주간 뮤직 차트 사이트\n",
    "url =   'https://music.bugs.co.kr/chart/track/week/total?chartdate=20240901'\n",
    "# 일간   https://music.bugs.co.kr/chart/track/day/total\n",
    "# 실시간 https://music.bugs.co.kr/chart/track/realtime/total\n",
    "\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, 'lxml')\n",
    "\n",
    "titles = soup_music.select('p.title a')\n",
    "# 노래제목 리스트 컴프리헨션\n",
    "music_titles = [title.get_text()  for title in titles ]\n",
    "\n",
    "titles = soup_music.select('p.artist a')\n",
    "# 가수 리스트 컴프리헨션\n",
    "music_artists = [artist.get_text()  for artist in titles ]\n",
    "\n",
    "music_titles_artistes= {}\n",
    "order = 0\n",
    "\n",
    "for (music_title, music_artist) in zip(music_titles, music_artists):\n",
    "    order = order + 1\n",
    "    music_titles_artistes[order] = [music_title, music_artist]\n",
    "\n",
    "music_titles_artistes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2032a-154a-4d09-bcfc-21b076ee988c",
   "metadata": {},
   "source": [
    "##### 날짜를 입력하면 벅스 차트에서 주간 음악순위 (1~100) 의 곡명과 아티스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0bb9fa4-9a9f-41a9-8ec4-1b51193ab871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def bugs_music_week_top100(year, month, day):\n",
    "    # 두자리로 \n",
    "    month = \"{0:02d}\".format(month)\n",
    "    day = \"{0:02d}\".format(day)\n",
    "\n",
    "    base_url = \"https://music.bugs.co.kr/chart/track/week/total?\"\n",
    "    url = base_url + 'chartdate={0}{1}{2}'.format(year, month, day)\n",
    "\n",
    "    html_music = requests.get(url).text\n",
    "    soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "    # 노래제목리스트와 가수리스트\n",
    "    titles = soup_music.select('p.title a')\n",
    "    artists = soup_music.select('p.artist a')\n",
    "\n",
    "    # 리스트 컴프리헨션\n",
    "    music_titles = [ title.get_text() for title in titles  ]\n",
    "    music_artists = [ artist.get_text() for artist in artists ]\n",
    "\n",
    "    return music_titles, music_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17a23cdc-6991-420f-9879-9da3941aa382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:/dev/myPyCode/data/bugs_week_top100_0901.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# 날짜 지정  함수 호출\n",
    "bugs_music_titles, bugs_music_artists = bugs_music_week_top100(2024, 9, 1)\n",
    "# 파일명 지정\n",
    "file_name = \"c:/dev/myPyCode/data/bugs_week_top100_0901.txt\"\n",
    "# 파일 열기\n",
    "f = open(file_name, \"w\")\n",
    "\n",
    "# 추출된 제목과 가수 파일에 저장\n",
    "for k in range(len(bugs_music_titles)):\n",
    "    f.write(\"{0:02d}위: {1} / {2}\\n\".format(k+1, bugs_music_titles[k].strip(), bugs_music_artists[k].strip()))\n",
    "\n",
    "f.close()\n",
    "\n",
    "glob.glob(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc77b5-6bbb-4cd6-9c01-3474b88d4584",
   "metadata": {},
   "source": [
    "##### 하나의 이미지 내려받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bce6231-0242-4e88-9448-6767b4484665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.python.org/static/img/python-logo.png\n",
    "import requests\n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'\n",
    "html_image = requests.get(url)\n",
    "html_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b8ab82-7cc6-4ae5-a09e-4f6667a6184b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-logo.png'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "image_file_name = os.path.basename(url)\n",
    "image_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5727f2b3-2e69-497b-84a2-7b5aa0b2cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"c:/dev/myPyCode/download\"\n",
    "if not os.path.exists(folder): # 폴더가 없다면\n",
    "    os.makedirs(folder)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f835df8-2323-47b4-9dfc-418a1cc31c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:/dev/myPyCode/download\\\\python-logo.png'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = os.path.join(folder, image_file_name)\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec9aa514-ccad-476f-ad40-b678aab26fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFile = open(image_path, \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8825ed42-5517-464d-bb9d-e17507f48a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터를 1000000바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "chunk_size = 1000000\n",
    "for chunk  in  html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e69eceb0-f008-4806-98c3-7d6196993c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python-logo.png']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7e33b-82db-4f5e-ab29-faecfbf8adc4",
   "metadata": {},
   "source": [
    "##### 여러 이미지 내려받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "954c1a45-5f4c-4d27-bac7-157eca01148f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"Twilight sky at han river in seoul city south Korea \" class=\"\" src=\"https://t4.ftcdn.net/jpg/02/58/00/87/360_F_258008704_rtbLWxLBUfMsryzYHEh7pfRkwEspn9XV.jpg\"/>,\n",
       " <img alt=\"Seoul Skyline\" class=\"\" src=\"https://t3.ftcdn.net/jpg/00/49/79/38/360_F_49793806_JutBMSah3cPh8IZkUiBh5xyOcW3IzRl8.jpg\"/>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://stock.adobe.com/kr/search/images?k=seoul'\n",
    "html_image_url = requests.get(url).text\n",
    "soup_image_url = BeautifulSoup(html_image_url, \"lxml\")\n",
    "\n",
    "# 요소 선택\n",
    "image_elements = soup_image_url.select(\"div.thumb-frame img\")\n",
    "image_elements[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93cd1e06-738d-4c5a-b8a6-a09ca3da0088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://t3.ftcdn.net/jpg/00/49/79/38/360_F_49793806_JutBMSah3cPh8IZkUiBh5xyOcW3IzRl8.jpg'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url = image_elements[1].get('src')   # 서버에 있는 이미지 위치\n",
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "817dceae-5c47-4eab-9218-f2da7d93dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "html_image = requests.get(image_url)\n",
    "\n",
    "# 이미지를 내려받을 폴더를 지정\n",
    "folder = \"c:/dev/myPyCode/download\"\n",
    "if not os.path.exists(folder): # 폴더가 없다면\n",
    "    os.makedirs(folder) \n",
    "\n",
    "image_path = os.path.join(folder, os.path.basename(image_url))\n",
    "imageFile = open(image_path, 'wb')\n",
    "\n",
    "# 이미지 데이터를 1000000바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "chunk_size = 1000000\n",
    "for chunk  in  html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)\n",
    "\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23d204-67fa-4be2-9f86-a1c2d9d0bf19",
   "metadata": {},
   "source": [
    "### 여러개 이미지 내려받기 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33675878-5ff8-4cdb-ba7a-3809b47d7fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_258008704_rtbLWxLBUfMsryzYHEh7pfRkwEspn9XV.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_49793806_JutBMSah3cPh8IZkUiBh5xyOcW3IzRl8.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_358322998_eWHzJmEnujVtb41bolba1Iz3tfIuCGYh.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_175434765_8BcLDKzt4asGu2TUe0EXkQZVGPBIQYOO.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_366876774_EbdjM6IVH9kwzw7ok9ZeqrHXkCkpJZhO.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_256858326_hZxyuIFLqaiokUHSduSs41pxeWj5EmAm.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_439454712_84RhEpQVA1FArMRyVh2igGFmTegNQSFP.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_272771902_1BQkZxSXAeero6EA8mY4Jm1tIzZ3qeqi.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_169519820_NbCSoI1VZHD4AnutZp6RNczt7zH9REfe.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_216782149_OWatL4mzpJkf4nrFolSpMg6Ac4ItIOhX.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_248435226_a53eGnKRposhGiI4OYBidQuPfvf0Jgaj.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_168060463_75EZgPPmmZNkutXBrz8wh0lzj7b3t7Jq.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_269407662_Czu9GSoGNEVXw6Fw9hxHL52ihkNb13a4.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_87149003_If8pQ7e0qnBKstTeZa5vzdY8fbxqIabR.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_297104397_bpgB7iNBuymzsOCm8JMj97sh0hwko9MG.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_174716684_e7B4fw0htYODHTovGGwHc8pNnPSRDmBh.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_333446294_AKnYIkIyTfBsxN5nj8I3o1PoT0xGa7Mb.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_287915462_bHsYaBbBLxveEegxI7BuS9rD0fo4pNjH.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_175984983_Ja1hRRH1NCuHs9bfntaXKGK5CyLnxiD7.jpg\n",
      "Image downloaded: c:/dev/myPyCode/download/seoul\\360_F_99746195_jJlwk4A8fb8GzLt7CabGygvwwwvfblLB.jpg\n",
      "==============================================\n",
      "원하는 갯수의 이미지 내려받기 완료!!!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# 1. img 태그 엘리먼트에서 src (이미지주소) 추출 함수\n",
    "def get_image_url(url):\n",
    "    # url에서 HTML 가져오기\n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")\n",
    "    \n",
    "    # div.thumb-frame 안의 img 태그 요소 선택\n",
    "    image_elements = soup_image_url.select('div.thumb-frame img')\n",
    "    \n",
    "    if image_elements:\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            img_src = image_element.get('src')\n",
    "            if img_src:  # None 값이 아닌 경우만 추가\n",
    "                image_urls.append(img_src)\n",
    "        return image_urls\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 2. img_folder: 내 로컬 PC에 저장할 폴더를 지정하고 이미지 주소에서 이미지 내려받기\n",
    "def download_image(img_folder, img_url):\n",
    "    if img_url:\n",
    "        # 이미지 파일 내려받기\n",
    "        html_image = requests.get(img_url)\n",
    "\n",
    "        # 폴더가 존재하지 않으면 폴더 생성\n",
    "        if not os.path.exists(img_folder):\n",
    "            os.makedirs(img_folder)\n",
    "\n",
    "        # 이미지 파일 이름과 저장 경로 설정\n",
    "        image_path = os.path.join(img_folder, os.path.basename(img_url))\n",
    "        \n",
    "        # 이미지 파일 쓰기 모드로 열고 저장\n",
    "        with open(image_path, 'wb') as imageFile:\n",
    "            imageFile.write(html_image.content)\n",
    "        \n",
    "        print(f\"Image downloaded: {image_path}\")\n",
    "    else:\n",
    "        print(\"No valid image URL provided\")\n",
    "\n",
    "# 3. 메인 실행 코드\n",
    "adobe_url = \"https://stock.adobe.com/kr/search/images?k=seoul\"\n",
    "\n",
    "# 내려받을 폴더 지정\n",
    "figure_folder = \"c:/dev/myPyCode/download/seoul\"\n",
    "\n",
    "# 이미지 파일 주소 리스트 가져오기\n",
    "adobe_image_urls = get_image_url(adobe_url)\n",
    "\n",
    "# 내려받을 이미지 개수 지정\n",
    "num_down_image_cnt = 20\n",
    "\n",
    "if adobe_image_urls:\n",
    "    for k in range(min(num_down_image_cnt, len(adobe_image_urls))):\n",
    "        download_image(figure_folder, adobe_image_urls[k])\n",
    "    print(\"==============================================\")\n",
    "    print(\"원하는 갯수의 이미지 내려받기 완료!!!\")\n",
    "else:\n",
    "    print(\"이미지 URL을 가져오지 못했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5356cb-a345-43b0-b2e4-c6515ec9fbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
